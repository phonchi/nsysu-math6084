{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"DC_Assignment1.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# Assignment 1"],"metadata":{"id":"ZguKQsaEtw45"}},{"cell_type":"markdown","source":["#### Student ID: *Double click here to fill the Student ID*\n","\n","#### Name: *Double click here to fill the name*"],"metadata":{"id":"RxvUeukG506E"}},{"cell_type":"markdown","source":["## Q1: Exploring the TensorFlow playground\n","\n","http://playground.tensorflow.org/\n","\n","(a) Execute the following steps first:\n","1. Change the dataset to exclusive OR dataset (top-right dataset under \"DATA\" panel). \n","2. Reduce the hidden layer to only one layer and change the activation function to \"ReLu\". \n","3. Run the model five times. Before each trial, hit the \"Reset the network\" button to get a new random initialization. (The \"Reset the network\" button is the circular reset arrow just to the left of the Play button.) \n","4. Let each trial run for at least 500 epochs to ensure convergence. \n","\n","Make some comments about the role of initialization in this non-convex optimization problem. What is the minimum number of neurons required (Keeping all other parameters unchanged) to ensure that it almost always converges to global minima (where the test loss is below 0.02)? Finally, paste the convergence results below.\n","\n","* Note the convergence results should include all the settings and the model. An example is available [here](https://drive.google.com/file/d/15AXYZLNMNnpZj0kI0CgPdKnyP_KqRncz/view?usp=sharing)"],"metadata":{"id":"FZWBn2fiCh0J"}},{"cell_type":"markdown","source":["<!---Your answer here.---!>"],"metadata":{"id":"73gcdAxzYkZq"}},{"cell_type":"markdown","source":["(b) Execute the following steps first\n","1. Change the dataset to be the spiral (bottom-right dataset under \"DATA\" panel). \n","2. Increase the noise level to 50 and leave the training and test set ratio unchanged. \n","3. Train the best model you can, using just `X1` and `X2` as input features. Feel free to add or remove layers and neurons. You can also change learning settings like learning rate, regularization rate, activations and batch size. Try to get the test loss below 0.15. \n","\n","How many parameters do you have in your models? Describe the model architecture and the training strategy you use. Finally, paste the convergence results below. \n","\n","* You may need to train the model for enough epochs here and use learning rate scheduling manually"],"metadata":{"id":"RR5jcFCZYTUW"}},{"cell_type":"markdown","source":["<!---Your answer here.---!>"],"metadata":{"id":"0d-AHbuMYcU-"}},{"cell_type":"markdown","source":["(c) Use the same dataset as described above with noise level set to 50. \n","This time, feel free to add additional features or other transformations like `sin(X1)` and `sin(X2)`. Again, try to get the loss below 0.15.\n","\n","Compare the results with (b) and describe your observation. Describe the model architecture and the training strategy you use. Finally, paste the convergence results below. "],"metadata":{"id":"DtU59aBRYTUX"}},{"cell_type":"markdown","source":["<!---Your answer here.---!>"],"metadata":{"id":"G0t84e3FY7wt"}},{"cell_type":"markdown","source":["## Q2: Takling MNIST with DNN"],"metadata":{"id":"4BHuiK8mtw5U"}},{"cell_type":"markdown","source":["In this question, we will explore the behavior of the vanishing gradient problem (which we have tried to solve using feature engineering in Q1) and try to solve it. The dataset we use is the famous MNIST dataset which contains ten different classes of handwritten digits. The MNIST database contains 60,000 training images and 10,000 testing images. In addition, each grayscale image is fit into a 28x28 pixel bounding box.\n","\n","http://yann.lecun.com/exdb/mnist/"],"metadata":{"id":"Tzp0m9WpYTUb"}},{"cell_type":"markdown","source":["(a) Load the MNIST dataset (you may refer to `keras.datasets.mnist.load_data()`), and split it into a training set (48,000 images), a validation set (12,000 images) and a test set (10,000 images). Make sure to standardize the dataset first."],"metadata":{"id":"Ngx3M8SStw5U"}},{"cell_type":"code","source":["# coding your answer here."],"metadata":{"id":"NaGlp4WvM1zU","execution":{"iopub.status.busy":"2022-02-27T13:08:48.116619Z","iopub.execute_input":"2022-02-27T13:08:48.117362Z","iopub.status.idle":"2022-02-27T13:08:49.254055Z","shell.execute_reply.started":"2022-02-27T13:08:48.117311Z","shell.execute_reply":"2022-02-27T13:08:49.253224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(b) Build a sequential model with 30 hidden dense layers (60 neurons each using ReLU as the activation function) plus an output layer (10 neurons using softmax as the activation function). Train it with SGD optimizer with learning rate 0.001 and momentum 0.9 for 10 epochs on MNIST dataset.  \n","\n","Try to manually calculate how many steps are in one epoch and compare it with the one reported by the program. Finally, plot the learning curves (loss vs epochs) and report the accuracy you get on the test set."],"metadata":{"id":"L3ZRsJXwtw5U"}},{"cell_type":"code","source":["# coding your answer here."],"metadata":{"id":"_12eDOV2tw5Z","execution":{"iopub.status.busy":"2022-02-27T13:09:55.677072Z","iopub.execute_input":"2022-02-27T13:09:55.677327Z","iopub.status.idle":"2022-02-27T13:11:01.943418Z","shell.execute_reply.started":"2022-02-27T13:09:55.677298Z","shell.execute_reply":"2022-02-27T13:11:01.942683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(c) Update the model in (b) to add a BatchNormalization (BN) layer after every hidden layer's activation functions. \n","\n","How do the training time and the performance compare with (b)? Try to manually calculate how many non-trainable parameters are in your model and compare it with the one reported by the program. Finally, try moving the BN layers before the hidden layers' activation functions and compare the performance with BN layers after the activation function."],"metadata":{"id":"HugAnsOHPKVt"}},{"cell_type":"code","source":["# coding your answer here."],"metadata":{"id":"dlLzZh5kO-5f","execution":{"iopub.status.busy":"2022-02-27T13:04:39.435768Z","iopub.execute_input":"2022-02-27T13:04:39.436025Z","iopub.status.idle":"2022-02-27T13:07:50.161991Z","shell.execute_reply.started":"2022-02-27T13:04:39.435997Z","shell.execute_reply":"2022-02-27T13:07:50.161271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Q3: High Accuracy CNN for CIFAR-10\n","When facing problems related to images like Q2, we can consider using CNN instead of DNN. The CIFAR-10 dataset is one of the most widely used datasets for machine learning research. It consists of 60000 32x32 color images in 10 classes, with 6000 images per class. In this problem, we will try to build our own CNN from scratch and achieve the highest possible accuracy on CIFAR-10. \n","\n","https://www.cs.toronto.edu/~kriz/cifar.html"],"metadata":{"id":"aBDkj0NJ18kk"}},{"cell_type":"markdown","source":["(a) Load the CIFAR10 dataset (you may refer to `keras.datasets.cifar10.load_data()`), and split it into a training set (40,000 images), a validation set (10,000 images) and a test set (10,000 images). Make sure the pixel values range from 0 to 1."],"metadata":{"id":"D_Gy-aD66KtE"}},{"cell_type":"code","source":["# coding your answer here."],"metadata":{"id":"jsVDZTvp6jQ4","execution":{"iopub.status.busy":"2022-02-27T11:20:20.953671Z","iopub.execute_input":"2022-02-27T11:20:20.953981Z","iopub.status.idle":"2022-02-27T11:20:31.295748Z","shell.execute_reply.started":"2022-02-27T11:20:20.953951Z","shell.execute_reply":"2022-02-27T11:20:31.294697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(b) Build a Convolutional Neural Network using the following architecture: \n","\n","|        | Type                | Maps    | Activation |\n","|--------|---------------------|---------|------------|\n","| Output | Fully connected     | 10      | Softmax    |\n","| S10    | Max Pooling         |         |            |\n","| B9     | Batch normalization |         |            |\n","| C8     | Convolution         | 64      | ReLu       |\n","| B7     | Batch normalization |         |            |\n","| C6     | Convolution         | 64      | ReLu       |\n","| S5     | Max Pooling         |         |            |\n","| B4     | Batch normalization |         |            |\n","| C3     | Convolution         | 32      | ReLu       |\n","| B2     | Batch normalization |         |            |\n","| C1     | Convolution         | 32      | ReLu       |\n","| In     | Input               | RGB (3) |            |\n","\n","Train the model for 20 epochs with NAdam optimizer (Adam with Nesterov momentum). \n","\n","Try to manually calculate the number of parameters in your model's architecture and compare it with the one reported by `summary()`. Finally, plot the learning curves and report the accuracy on the test set."],"metadata":{"id":"8fSsEjKA88Jc"}},{"cell_type":"code","source":["# coding your answer here."],"metadata":{"id":"A26QxIhi2DuQ","execution":{"iopub.status.busy":"2022-02-27T11:59:39.306471Z","iopub.execute_input":"2022-02-27T11:59:39.306816Z","iopub.status.idle":"2022-02-27T11:59:39.438328Z","shell.execute_reply.started":"2022-02-27T11:59:39.306782Z","shell.execute_reply":"2022-02-27T11:59:39.437134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(c) Looking at the learning curves, you can see that the model is overfitting. Adding data augmentation layer for the model in (a) as follows.\n","\n","* Applies random horizontal flipping \n","* Rotates the input images by a random value in the range `[â€“18 degrees, +18 degrees]`)\n","* Zooms in or out of the image by a random factor in the range `[-15%, +15%]`\n","* Randomly choose a location to crop images down to a target size `[30, 30]`\n","* Randomly adjust the contrast of images so that the resulting images are `[0.9, 1.1]` brighter or darker than the original one.\n","\n","Fit your model for enough epochs (60, for instance) and compare its performance and learning curves with the previous model in (b). Finally, report the accuracy on the test set.\n"],"metadata":{"id":"X1TICQ99_TbP"}},{"cell_type":"code","source":["# coding your answer here."],"metadata":{"id":"ZmarpSop_h3Y","execution":{"iopub.status.busy":"2022-02-27T11:39:33.833007Z","iopub.execute_input":"2022-02-27T11:39:33.833785Z","iopub.status.idle":"2022-02-27T11:39:33.868600Z","shell.execute_reply.started":"2022-02-27T11:39:33.833738Z","shell.execute_reply":"2022-02-27T11:39:33.867590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(d) Replace all the convolution layers in (b) with depthwise separable convolution layers (except the first convolution layer).  \n","\n","Try to manually calculate the number of parameters in your model's architecture and compare it with the one reported by `summary()`. Fit your model and compare its performance with the previous model in (c). Finally, plot the learning curves and report the accuracy on the test set."],"metadata":{"id":"Ty5y7QjR_jA0"}},{"cell_type":"code","source":["# coding your answer here."],"metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:05:36.733981Z","iopub.execute_input":"2022-02-27T12:05:36.734356Z","iopub.status.idle":"2022-02-27T12:05:37.101399Z","shell.execute_reply.started":"2022-02-27T12:05:36.734321Z","shell.execute_reply":"2022-02-27T12:05:37.100297Z"},"trusted":true,"id":"WCBq_J1mYTUi"},"execution_count":null,"outputs":[]}]}