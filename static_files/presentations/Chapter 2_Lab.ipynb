{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"Chapter 2_Lab.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ib9xIDnjTZvs"},"source":["# Chapter 2 - Statistical Learning Laboratory"]},{"cell_type":"code","metadata":{"id":"RuzJlGP1TZvv"},"source":["# imports and setup\n","import pandas as pd\n","import numpy as np\n","from scipy.stats.stats import pearsonr\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D # for 3D plots\n","import seaborn as sns\n","\n","import math\n","\n","\n","\n","%matplotlib inline\n","pd.set_option('precision', 2) # number precision for pandas\n","plt.style.use('seaborn-white')\n","sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jADSLEnbTZvw"},"source":["## Basic Commands\n","Refer to `Numpy` notebook for more detail"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-06-13T20:01:46.011347Z","start_time":"2017-06-13T20:01:45.997858Z"},"id":"3QC35D-YTZvw"},"source":["# vector\n","x = [1,3,2,5] # do not use this\n","x = np.array([1, 6, 2]) # use ndarray instead\n","y = np.array([1, 4, 3])\n","x.shape, y.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-06-13T20:01:46.024568Z","start_time":"2017-06-13T20:01:46.015380Z"},"id":"vZ2UuiPgTZvx"},"source":["# array operations\n","x + y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-06-13T20:29:05.439139Z","start_time":"2017-06-13T20:29:05.429659Z"},"id":"thmX7NCNTZvy"},"source":["# matrix creation\n","x = np.array([[1,2],[3,4]]) # rwo major, do not use np.matrix\n","x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X-wJNWtrTZvz"},"source":["x = np.asfortranarray(np.array(range(1,5)).reshape(2,2)) #do not do this\n","x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xgvzd7DZTZv0"},"source":["np.array(range(1,5)).reshape(2,2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-06-13T20:29:10.625349Z","start_time":"2017-06-13T20:29:10.617623Z"},"id":"o3r-WwruTZv1"},"source":["#matrix operations\n","np.sqrt(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-06-13T20:29:10.625349Z","start_time":"2017-06-13T20:29:10.617623Z"},"id":"7zL5cfDYTZv2"},"source":["np.power(x, 2), x**2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-06-13T20:01:46.060277Z","start_time":"2017-06-13T20:01:46.050870Z"},"id":"X_bm1VaxTZv3"},"source":["# random normal distribution & correlation\n","x = np.random.randn(50)\n","y = x + np.random.normal(loc=50, scale=.1, size=50)\n","pearsonr(x, y)[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-06-13T20:01:46.073053Z","start_time":"2017-06-13T20:01:46.063246Z"},"id":"TXiUpU09TZv3"},"source":["# random seed and basic statistical functions\n","np.random.seed(3)\n","y = np.random.randn(50)\n","y.mean(), y.var(), np.sqrt(y.var()), y.std()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cd4X8ZHvTZv4"},"source":["## Graphics"]},{"cell_type":"markdown","metadata":{"id":"6ECKgdxTTZv4"},"source":["Here, we review some of the command we have learned using `Seaborn` "]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-06-13T20:06:33.179069Z","start_time":"2017-06-13T20:06:32.737546Z"},"id":"-J8hG2FOTZv5"},"source":["x = np.random.randn(100)\n","y = np.random.randn(100)\n","\n","# seaborn scatterplot\n","p = sns.jointplot(x=x, y=y, kind='scatter')\n","p.set_axis_labels(xlabel='this is x axis', ylabel='this is y axis')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MyfbCfurTZv5"},"source":["We will often want to save the output of an `Python` plot. The command that we use to do this will be the `savefig()`."]},{"cell_type":"code","metadata":{"id":"j-VLQP41TZv5"},"source":["p.savefig(\"Figure.pdf\") #depends on the file extension\n","p.savefig(\"Figure.jpg\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-06-13T20:01:46.522155Z","start_time":"2017-06-13T20:01:46.517632Z"},"id":"eQsVs_EBTZv6"},"source":["# create a sequence of numbers\n","x = np.arange(1, 11)\n","x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-06-13T20:07:06.515679Z","start_time":"2017-06-13T20:07:06.503578Z"},"id":"rvsBAB6UTZv6"},"source":["# linearly spaced numbers\n","x = np.linspace(-np.pi, np.pi, num=50)\n","x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T-u6kd3mTZv6"},"source":["We  will now create some more sophisticated plots. We create three array here\n"," \n"," * A vector of the `x` values (the first dimension),\n"," * A vector of the `y` values (the second dimension), and\n"," * An 2D array whose elements correspond to the `z` value (the third dimension) for each pair of (`x`, `y`) coordinates."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-06-13T20:08:46.572575Z","start_time":"2017-06-13T20:08:45.966004Z"},"id":"7O8-EZ1ATZv7"},"source":["plt.figure(figsize=(6,6))\n","\n","x = np.linspace(-np.pi, np.pi, num=50)\n","y = x\n","\n","# simulating R outer function\n","def pf(a, b):\n","    return math.cos(b) / (1 + a**2)\n","\n","f = np.empty((len(x), len(y)))\n"," \n","for i in range(len(x)):\n","    for j in range(len(y)):\n","        f[i,j] = pf(x[i], y[j])\n","\n","        \n","# contour plot\n","cp = plt.contour(x, y, f.T, 45, cmap='viridis')\n","plt.clabel(cp, inline=1, fontsize=10);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-06-13T20:08:54.978427Z","start_time":"2017-06-13T20:08:54.614479Z"},"id":"WSqmvjOvTZv7"},"source":["# contour 2\n","plt.figure(figsize=(6,6))\n","\n","fa = (f - f.T)/2\n","cp = plt.contour(x, y, fa.T, 15, cmap='viridis')\n","plt.clabel(cp, inline=1, fontsize=10);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-06-13T20:08:58.739610Z","start_time":"2017-06-13T20:08:58.308899Z"},"id":"4yxQK19VTZv7"},"source":["# heatmap\n","plt.figure(figsize=(6,6))\n","cp = plt.contourf(x, y, fa.T, 15, cmap='viridis') #adiidtional f\n","plt.clabel(cp, inline=1, fontsize=10)\n","plt.colorbar()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-06-13T20:09:59.159079Z","start_time":"2017-06-13T20:09:58.865435Z"},"scrolled":true,"id":"SI5AcLHBTZv8"},"source":["# 3d perspective\n","\n","fig = plt.figure(figsize=(10,10))\n","ax = fig.add_subplot(111, projection='3d')\n","ax.plot_wireframe(x, y, fa.T, cmap='viridis')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2017-06-13T19:57:49.908623Z","start_time":"2017-06-13T19:57:36.226838Z"},"id":"VjcX4OJRTZv8"},"source":["## Indexing Data"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-06-13T20:29:24.614702Z","start_time":"2017-06-13T20:29:24.605374Z"},"id":"4EvHjFokTZv8"},"source":["# matrix creation (R equivalent of matrix(1:16, 4 ,4))\n","A = np.arange(16).reshape(4, 4).transpose()\n","A"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-06-13T20:31:20.667992Z","start_time":"2017-06-13T20:31:20.660999Z"},"id":"l7oxIbsrTZv8"},"source":["A[1, 2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-06-13T20:35:28.030453Z","start_time":"2017-06-13T20:35:28.021984Z"},"id":"LFIPmGuuTZv9"},"source":["# select a range of rows and columns\n","A[0:3, 1:4]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-06-13T20:36:50.623704Z","start_time":"2017-06-13T20:36:50.616319Z"},"id":"PDfXXCdQTZv9"},"source":["# select a range of rows and all columns\n","A[0:2,:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-06-13T20:36:52.631832Z","start_time":"2017-06-13T20:36:52.624482Z"},"id":"OtzVP5BATZv9"},"source":["# select all rows and a range of columns\n","A[:,0:2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1BOgFe0jTZv9"},"source":["A[0,:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-06-13T20:31:06.168934Z","start_time":"2017-06-13T20:31:06.159935Z"},"scrolled":true,"id":"GLHt27V5TZv-"},"source":["# shape of the matrix\n","A.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kVHxoburTZv-"},"source":["## Loading Data"]},{"cell_type":"markdown","metadata":{"id":"Riizh6l6TZv-"},"source":["The data set also includes a number of missing observations, indicated by a question mark `?`. Missing values are a common occurrence in real data sets."]},{"cell_type":"code","metadata":{"id":"6UEjEvfgXNps"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-06-13T20:44:29.152873Z","start_time":"2017-06-13T20:44:29.080324Z"},"id":"DigwbfgbTZv-"},"source":["# read csv data with pandas into dataframe, explicitly setting na_values.\n","# pandas read_xxx functions infer datatypes, headers, dates, etc. \n","# without explicit declarations\n","Auto = pd.read_csv('/content/drive/MyDrive/NSYSU/00_Statistical_learning/Lab/Data/Auto_origin.csv', na_values=['?'])\n","Auto"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TlKmw06XTZv_"},"source":["# dropping rows (axis-0) where there are NA values (inplace)\n","Auto.dropna(axis=0, inplace=True)\n","Auto.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-06-13T20:44:19.762836Z","start_time":"2017-06-13T20:44:19.756495Z"},"id":"TVryDVsYTZv_"},"source":["Auto.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_5o5Vib4TZv_"},"source":["Auto.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-06-13T20:45:47.914646Z","start_time":"2017-06-13T20:45:47.907574Z"},"id":"3HU0BUdgTZwA"},"source":["# get column names of the dataframe\n","Auto.columns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-06-13T20:47:47.796008Z","start_time":"2017-06-13T20:47:47.325436Z"},"id":"F355iC6NTZwA"},"source":["# seaborn scatterplot\n","pl = sns.jointplot(x='cylinders', y='mpg', data=Auto)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2017-06-13T20:49:30.154399Z","start_time":"2017-06-13T20:49:30.147108Z"},"scrolled":true,"id":"I74JsJFDTZwA"},"source":["# changing data type of a column into category\n","Auto['cylinders'] = Auto['cylinders'].astype('category')\n","Auto.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yj6EQSR_TZwA"},"source":["# seaborn boxplot \n","sns.boxplot(x='cylinders', y='mpg', data=Auto);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fTY5FuoUTZwA"},"source":["# seaborn displot\n","sns.displot(x=\"mpg\", data=Auto, bins=15)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MtuaRKTVTZwB"},"source":["# seaborn pairplot for selected variables, colored by another\n","sns.pairplot(Auto, vars=['mpg', 'displacement', 'horsepower', 'weight', 'acceleration'], hue='cylinders')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c6NvtZmUTZwB"},"source":["# summary statistics for all dataframe columns, including non-numerical columns\n","Auto.describe(include='all')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YesoDbR3TZwB"},"source":["# summary statistics for a single column and wrapped as dataframe for pretty table display in jupyter\n","pd.DataFrame(Auto['mpg'].describe())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7fNg3kgWZFbD"},"source":["## KNN example"]},{"cell_type":"markdown","metadata":{"id":"KfFbLvN8ZJSB"},"source":["The following code is modified from https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks\n","\n","1. Generates 10 means $m_k$ from a bivariate Gaussian distrubition for each color:\n","   - $N((1, 0)^T, \\textbf{I})$ for <span style=\"color: blue\">BLUE</span>\n","   - $N((0, 1)^T, \\textbf{I})$ for <span style=\"color: orange\">ORANGE</span>\n","2. For each color generates 100 observations as following:\n","   - For each observation it picks $m_k$ at random with probability 1/10.\n","   - Then generates a $N(m_k,\\textbf{I}/5)$"]},{"cell_type":"code","metadata":{"id":"gnzg_LIRaFP8"},"source":["from sklearn.mixture import GaussianMixture\n","from sklearn.mixture.gaussian_mixture import _compute_precision_cholesky\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import accuracy_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XdJrTMY6aHHt"},"source":["# load training data that was used in the book\n","df = pd.read_csv(\"/content/drive/MyDrive/NSYSU/00_Statistical_learning/Lab/Data/mixture.txt\")\n","X_train = df[['x1', 'x2']].values\n","y_train = df.y.values\n","\n","# set the known BLUE and ORANGE clusters means\n","blue_means = np.array([[-0.25343316, 1.7414788], [0.26669318, 0.3712341],\n","                       [2.09646921, 1.2333642], [-0.06127272, -0.2086791],\n","                       [2.70354085, 0.5968283], [2.37721198, -1.1864147],\n","                       [1.05690759, -0.6838939], [0.57888354, -0.0683458],\n","                       [0.62425213, 0.5987384], [1.67335495, -0.2893159]])\n","orange_means = np.array([[1.19936869, 0.2484086], [-0.30256110, 0.9454190],\n","                         [0.05727232, 2.4197271], [1.32932203, 0.8192260],\n","                         [-0.07938424, 1.6138017], [3.50792673, 1.0529863],\n","                         [1.61392290, 0.6717378], [1.00753570, 1.3683071],\n","                         [-0.45462141, 1.0860697], [-1.79801805, 1.9297806]])\n","all_means = np.vstack((blue_means, orange_means))\n","\n","gaussian_mixture_model = GaussianMixture(\n","    n_components=20,\n","    covariance_type='spherical',\n","    means_init=all_means,\n","    random_state=1\n",").fit(all_means)\n","# set known covariances\n","gaussian_mixture_model.covariances_ = [1/5]*20\n","# it looks like a hack, but GaussianMixture uses precisions_cholesky_\n","# for predict_proba method. Because we changed covariances_ we need\n","# to recalculate precisions_cholesky_ too.\n","gaussian_mixture_model.precisions_cholesky_ = _compute_precision_cholesky(\n","    gaussian_mixture_model.covariances_,\n","    gaussian_mixture_model.covariance_type)\n","\n","# sample 10,000 points for testing\n","X_test, y_test = gaussian_mixture_model.sample(10000)\n","# y_test contains sampled component indices\n","# index < 10 means that the class is BLUE (0)\n","y_test = 1*(y_test >= 10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DX4rw7NAeDm0"},"source":["def optimal_bayes_predict(X):\n","    components_proba = gaussian_mixture_model.predict_proba(X)\n","    # first 10 components are BLUE(0), and others are BROWN(1)\n","    blue_proba = np.sum(components_proba[:, :10], axis=1)\n","    brown_proba = np.sum(components_proba[:, 10:], axis=1)\n","    y_hat = 1*(blue_proba < brown_proba)\n","    return y_hat"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dEqW2uUZggJz"},"source":["bayes_error_rate = 1 - accuracy_score(y_train, optimal_bayes_predict(X_train))\n","print(f'The optimal Bayes error rate = {bayes_error_rate}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bd9S9QYOeGar"},"source":["bayes_error_rate = 1 - accuracy_score(y_test, optimal_bayes_predict(X_test))\n","print(f'The optimal Bayes error rate = {bayes_error_rate}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TQd26QFTeH-b"},"source":["# define commonly used colors\n","GRAY1, GRAY4, PURPLE = '#231F20', '#646369', '#A020F0'\n","BLUE, ORANGE, BLUE1 = '#57B5E8', '#E69E00', '#174A7E'\n","\n","# configure all plots font family and border line widths\n","plt.rcParams['font.family'] = 'Arial'\n","plt.rcParams['axes.linewidth'] = 0.5\n","\n","# prepares a plot with a title and circles representing training data\n","def plot_train_data(title):\n","    fig, ax = plt.subplots(figsize=(2.8, 2.8), dpi=110)\n","    ax.set_aspect(1.3)\n","    ax.scatter(X_train[:, 0], X_train[:, 1], s=18, facecolors='none',\n","               edgecolors=np.array([BLUE, ORANGE])[y_train])\n","    ax.tick_params(\n","        bottom=False, left=False, labelleft=False, labelbottom=False)\n","    ax.set_xlim(-2.6, 4.2)\n","    ax.set_ylim(-2.0, 2.9)\n","    fig.subplots_adjust(left=0, right=1, top=1, bottom=0)\n","    ax.text(-2.6, 3.2, title, color=GRAY4, fontsize=9)\n","    for spine in ax.spines.values():\n","        spine.set_color(GRAY1)\n","    return fig, ax\n","\n","# test it\n","_, _ = plot_train_data('Training data')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VtD2kpBvemw9"},"source":["# given a model prediction function computes X points on n x n grid and the\n","# corresponding predicted classes\n","def fill_prediction_grid(n1, n2, predict):\n","    x1, x2 = np.linspace(-2.6, 4.2, n1), np.linspace(-2.0, 2.9, n2)\n","    X = np.transpose([np.tile(x1, n2), np.repeat(x2, n1)])\n","    y = predict(X)\n","    return X, y\n","\n","\n","# given a model prediction function computes X0 and X1 n x n meshgrids\n","# and the corresponing predicted classes meshgrid\n","def fill_prediction_meshgrid(predict):\n","    n1, n2 = 1000, 1000\n","    X, y = fill_prediction_grid(n1, n2, predict)\n","    return X[:, 0].reshape(n1, n2), X[:, 1].reshape(n1, n2), y.reshape(n1, n2)\n","\n","\n","# given a model prediction function plots train data, model decision\n","# bounary and background dots\n","def plot_model(predict, title):\n","    fig, ax = plot_train_data(title)\n","    # plot background dots\n","    X, y = fill_prediction_grid(69, 99, predict)\n","    ax.scatter(X[:, 0], X[:, 1], marker='.', lw=0, s=2,\n","               c=np.array([BLUE, ORANGE])[y])\n","    # plot the decision boundary\n","    X0, X1, Y = fill_prediction_meshgrid(predict)\n","    ax.contour(X0, X1, Y, [0.5], colors=GRAY1, linewidths=[0.7])\n","    return fig, ax\n","\n","# plot the optimal Bayes decision boundary\n","_, _ = plot_model(optimal_bayes_predict, 'Bayes Optimal Classifier')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QWi36wZxexW8"},"source":["# lets save Bayes meshgrids for optimal decision boundary plotting\n","X0_bayes, X1_bayes, Y_bayes = fill_prediction_meshgrid(optimal_bayes_predict)\n","\n","\n","# given a model prediction function plots performance statistics\n","def plot_model_stat(predict, title):\n","    fig, ax = plot_model(predict, title)\n","    ax.contour(X0_bayes, X1_bayes, Y_bayes, [0.5], colors='purple',\n","               linewidths=[0.5], linestyles='dashed')\n","    test_error_rate = 1 - accuracy_score(y_test, predict(X_test))\n","    train_error_rate = 1 - accuracy_score(y_train, predict(X_train))\n","    parms = {'color': GRAY1, 'fontsize': 7,\n","             'bbox': {'facecolor': 'white', 'pad': 3, 'edgecolor': 'none'}}\n","    ax.text(-2.42, -1.35, f'Training Error: {train_error_rate:.3f}', **parms)\n","    ax.text(-2.42, -1.62, f'Test Error:       {test_error_rate:.3f}', **parms)\n","    ax.text(-2.42, -1.89, f'Bayes Error:    {bayes_error_rate:.3f}', **parms)\n","    return fig, ax"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v03c1yfYfASU"},"source":["# Run GridSearchCV to find the best n_neighbors parameter using the 10-folds\n","# CV. It finds 12, but the book uses 15-Nearest Neighbor Classifier because\n","# the authors selected the most parsimonious model within one standard error\n","# from the best model (one standard error rule). We will apply this rule in\n","# other examples, not here.\n","k_neighbors_grid_search = GridSearchCV(\n","    KNeighborsClassifier(),\n","    {'n_neighbors': list(range(1, 50))},\n","    cv=10\n",").fit(X_train, y_train)\n","k_neighbors_grid_search.best_params_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8eXaNFnvfZxb"},"source":["# PAGE 14. Use 15-nearest-neighbor averaging of the binary coded response as\n","#          the method of fitting. Thus Y-hat is the proportion of ORANGE’s in\n","#          the neighborhood, and so assigning class ORANGE to G-hat if\n","#          Y-hat>0.5 amounts to a majority vote in the neighborhood.\n","neighbors15_classifier = KNeighborsClassifier(\n","    n_neighbors=15\n",").fit(X_train, y_train)\n","_, _ = plot_model_stat(\n","    neighbors15_classifier.predict, '15-Nearest Neighbor Classifier')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-XuND_fifchs"},"source":["# PAGE 16. The classes are coded as a binary variable (BLUE = 0,ORANGE = 1),\n","#          and then predicted by 1-nearest-neighbor classiﬁcation.\n","neighbors1_classifier = KNeighborsClassifier(\n","    n_neighbors=1\n",").fit(X_train, y_train)\n","_, _ = plot_model_stat(\n","    neighbors1_classifier.predict, '1−Nearest Neighbor Classifier')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wXL6l_TyfgsE"},"source":["n_neighbors_vals = list(range(1, 30, 2))\n","k_neighbors_grid_search = GridSearchCV(\n","    KNeighborsClassifier(),\n","    {'n_neighbors': n_neighbors_vals},\n","    cv=10, scoring='accuracy',\n","    return_train_score=True, iid=True\n",").fit(X_train, y_train)\n","\n","train_errors, test_errors = [], []\n","for k in n_neighbors_vals:\n","    clf = KNeighborsClassifier(n_neighbors=k).fit(X_train, y_train)\n","    train_errors.append(1 - clf.score(X_train, y_train))\n","    test_errors.append(1 - clf.score(X_test, y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ATpbz0tJfjzD"},"source":["# PAGE 467. k-nearest-neighbors on the two-class mixture data. The upper\n","#           panel shows the misclassification errors as a function of\n","#           neighborhood size. Standard error bars are included for 10-fold\n","#           cross validation. The lower panel shows the decision boundary\n","#           for 7-nearest-neighbors, which appears to be optimal for minimizing\n","#           test error.\n","cv_erros = 1 - np.vstack([\n","    k_neighbors_grid_search.cv_results_[f'split{i}_test_score']\n","    for i in range(10)]).T\n","cv_mean_errors = np.mean(cv_erros, axis=1)\n","cv_std_errors = np.std(cv_erros, ddof=1, axis=1)/np.sqrt(10)\n","best_index = np.argmin(cv_mean_errors)\n","best_err, best_std_err = cv_mean_errors[best_index], cv_std_errors[best_index]\n","\n","fig, ax = plt.subplots(figsize=(2.8, 2.8), dpi=110)\n","fig.subplots_adjust(left=0, right=1, top=1, bottom=0)\n","ax.scatter(n_neighbors_vals, test_errors, c='#0000FF', s=9)\n","ax.plot(n_neighbors_vals, test_errors, c='#0000FF', linewidth=0.8,\n","        label='Test Error')\n","ax.plot(n_neighbors_vals, cv_mean_errors, c='#00FF00', linewidth=0.8,\n","        label='10-fold CV')\n","ax.scatter(n_neighbors_vals, train_errors, c=ORANGE, s=9)\n","ax.plot(n_neighbors_vals, train_errors, c=ORANGE, linewidth=0.8,\n","        label='Training Error')\n","\n","ax.errorbar(n_neighbors_vals, cv_mean_errors,\n","            color='#00FF00', linestyle='None', marker='o', elinewidth=0.8,\n","            markersize=3, yerr=cv_std_errors, ecolor='#00FF00', capsize=3)\n","ax.axhline(y=best_err+best_std_err, c=PURPLE, linewidth=0.8, linestyle='--',\n","           label='Bayes Error')\n","for i in ax.get_yticklabels() + ax.get_xticklabels():\n","    i.set_fontsize(6)\n","ax.set_xlabel('Number of Neighbors', color=GRAY4, fontsize=8)\n","ax.set_ylabel('Misclassification Errors', color=GRAY4, fontsize=8)\n","\n","neighbors7_classifier = KNeighborsClassifier(\n","    n_neighbors=7).fit(X_train, y_train)\n","plot_model_stat(neighbors7_classifier.predict, '7-Nearest Neighbors')\n","_ = ax.legend(loc='bottom right', prop={'size': 8})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PZvOx1oBjW34"},"source":["References\n","\n","[1] https://web.stanford.edu/~hastie/ElemStatLearn/\n","\n","[2] https://github.com/emredjan/ISL-python\n","\n","[3] https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks"]}]}